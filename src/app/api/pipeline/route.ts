/**
 * POST /api/pipeline
 * 
 * ìœ íŠœë¸Œ ì‡¼ì¸  ì˜ìƒ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘ API
 * 
 * 1. ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (OpenRouter ë˜ëŠ” Google AI)
 * 2. ìŒì„± ìƒì„± (ElevenLabs TTS)
 * 3. ì˜ìƒ ë Œë”ë§ ìš”ì²­ (Runway ML ë˜ëŠ” Google Imagen + FFmpeg)
 */

import { NextRequest, NextResponse } from 'next/server';
import { generateScript as generateScriptGoogle } from '@/lib/llm';
import { generateScript as generateScriptOpenRouter } from '@/lib/openrouter';
import { generateTTS } from '@/lib/elevenlabs';
import { saveAudio } from '@/lib/storage';
import { requestBrewVideo as requestRunwayVideo } from '@/lib/runway';
import { requestGoogleVideo } from '@/lib/google-video';
import { requestKlingVideo } from '@/lib/kling';
import { requestVeoVideo } from '@/lib/veo';
import { requestHiggsfieldVideo } from '@/lib/higgsfield';
import { 
  initializeSteps, 
  updateStep, 
  splitScriptWithDurations, 
  initializeClipsFromSections,
  generateClipPrompts,
  requestClipVideos 
} from '@/lib/multi-clip';
import { getAudioDuration } from '@/lib/ffmpeg';
import { createJob, updateJobStatus } from '@/lib/jobs';
import type { PipelineRequest, PipelineResponse, ApiErrorResponse, VideoProvider, AIProvider, VeoModel, HiggsfieldModel, MultiClipStep, ClipInfo } from '@/types';

export async function POST(request: NextRequest) {
  try {
    // ìš”ì²­ íŒŒì‹±
    const body = await request.json() as PipelineRequest;
    const { 
      topic, 
      category, 
      voiceId, 
      autoUpload = true, 
      videoProvider = 'veo',  // ê¸°ë³¸ê°’: Veo3
      aiProvider = 'openrouter'  // ê¸°ë³¸ê°’: OpenRouter (ë‹¤ì–‘í•œ AI í™œìš©)
    } = body;
    
    // Veo ëª¨ë¸ ì„ íƒ (ìš”ì²­ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’)
    const veoModel = (body as any).veoModel as VeoModel | undefined;
    // Veo ì˜ìƒ ê¸¸ì´ (ì´ˆ)
    const veoDuration = (body as any).veoDuration as number | undefined;
    // Higgsfield ëª¨ë¸ ì„ íƒ (ìš”ì²­ì—ì„œ ê°€ì ¸ì˜¤ê±°ë‚˜ ê¸°ë³¸ê°’)
    const higgsfieldModel = (body as any).higgsfieldModel as HiggsfieldModel | undefined;
    // ë©€í‹°í´ë¦½ ëª¨ë“œ
    const useMultiClip = (body as any).useMultiClip as boolean | undefined;
    const targetDuration = (body as any).targetDuration as number | undefined;
    // Kling ì˜ìƒ ê¸¸ì´
    const klingDuration = (body as any).klingDuration as '5' | '10' | undefined;

    // ê¸°ë³¸ ê²€ì¦
    if (!topic || typeof topic !== 'string') {
      return NextResponse.json<ApiErrorResponse>(
        { success: false, error: 'ì£¼ì œ(topic)ëŠ” í•„ìˆ˜ì…ë‹ˆë‹¤.' },
        { status: 400 }
      );
    }

    if (topic.length < 2 || topic.length > 200) {
      return NextResponse.json<ApiErrorResponse>(
        { success: false, error: 'ì£¼ì œëŠ” 2ì ì´ìƒ 200ì ì´í•˜ì—¬ì•¼ í•©ë‹ˆë‹¤.' },
        { status: 400 }
      );
    }

    // Veo ìŒì„± í¬í•¨ ì˜µì…˜ (Veoê°€ ìŒì„±ë„ ìƒì„±)
    const veoWithAudio = (body as any).veoWithAudio as boolean | undefined;

    // Step 1: Job ìƒì„±
    const job = await createJob({
      topic,
      category,
      autoUpload,
    });

    console.log(`[Pipeline] Job ìƒì„± ì™„ë£Œ: ${job.id}, AI: ${aiProvider}, Video: ${videoProvider}, VeoWithAudio: ${veoWithAudio}`);

    try {
      // Step 2: ìŠ¤í¬ë¦½íŠ¸ ìƒì„± (AI Providerì— ë”°ë¼ ë¶„ê¸°)
      console.log(`[Pipeline] ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì‹œì‘ (${aiProvider})...`);
      
      let script: string;
      if (aiProvider === 'openrouter') {
        // OpenRouter: Claude, GPT, Gemini ë“± ìƒí™©ë³„ ìµœì  ëª¨ë¸ ì‚¬ìš©
        script = await generateScriptOpenRouter({ topic, category });
      } else {
        // Google AI (ê¸°ì¡´ ë°©ì‹)
        script = await generateScriptGoogle({ topic, category });
      }
      
      await updateJobStatus(job.id, {
        status: 'audio',
        script,
      });
      console.log(`[Pipeline] ìŠ¤í¬ë¦½íŠ¸ ìƒì„± ì™„ë£Œ (${script.length}ì)`);

      let audioUrl: string | undefined;
      let audioPath: string | undefined;

      // Veo + ìŒì„± í¬í•¨ ëª¨ë“œ: TTS ê±´ë„ˆë›°ê³  Veoê°€ ìŒì„± ìƒì„±
      if (videoProvider === 'veo' && veoWithAudio) {
        console.log(`[Pipeline] Veo ìŒì„± í¬í•¨ ëª¨ë“œ - TTS ê±´ë„ˆë›°ê¸°`);
        await updateJobStatus(job.id, {
          status: 'render',
        });
      } else {
        // ê¸°ì¡´ ë°©ì‹: TTS ìŒì„± ìƒì„±
        // Step 3: TTS ìŒì„± ìƒì„±
        console.log(`[Pipeline] TTS ìƒì„± ì‹œì‘...`);
        const { audioBuffer, fileName } = await generateTTS({
          script,
          voiceId,
        });

        // Step 4: ì˜¤ë””ì˜¤ íŒŒì¼ ì €ì¥
        const savedAudio = await saveAudio({
          buffer: audioBuffer,
          fileName,
        });
        audioUrl = savedAudio.url;
        audioPath = savedAudio.path;

        await updateJobStatus(job.id, {
          status: 'render',
          audioUrl,
        });
        console.log(`[Pipeline] TTS ì €ì¥ ì™„ë£Œ: ${audioUrl}`);
      }

      // Step 5: ì˜ìƒ ìƒì„± ìš”ì²­ (Providerì— ë”°ë¼ ë¶„ê¸°)
      let brewJobId: string;
      let providerName: string;

      if (videoProvider === 'higgsfield') {
        // Higgsfield ë°©ì‹ (ì—¬ëŸ¬ AI ëª¨ë¸ í†µí•©)
        const selectedModel = higgsfieldModel || 'veo-3.1';
        
        if (useMultiClip && targetDuration && targetDuration > 15 && audioPath) {
          // ============================================
          // ğŸ¬ ë©€í‹°í´ë¦½ ëª¨ë“œ: ìƒˆ ì›Œí¬í”Œë¡œìš°
          // ============================================
          console.log(`[Pipeline] ë©€í‹°í´ë¦½ ëª¨ë“œ ì‹œì‘ (ëª©í‘œ: ${targetDuration}ì´ˆ, ëª¨ë¸: ${selectedModel})...`);
          providerName = `Higgsfield MultiClip (${selectedModel})`;
          
          // Step 1: ì‘ì—… ë‹¨ê³„ ì´ˆê¸°í™”
          let steps = initializeSteps();
          steps = updateStep(steps, 'script', { status: 'completed', endTime: new Date().toISOString() });
          steps = updateStep(steps, 'tts', { status: 'completed', endTime: new Date().toISOString() });
          
          // Step 2: ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
          steps = updateStep(steps, 'split', { status: 'processing', startTime: new Date().toISOString() });
          console.log(`[Pipeline] ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì¤‘...`);
          const audioDuration = await getAudioDuration(audioPath);
          console.log(`[Pipeline] ì˜¤ë””ì˜¤ ê¸¸ì´: ${audioDuration}ì´ˆ`);
          
          // Step 3: ìŠ¤í¬ë¦½íŠ¸ ë¶„í•  ë° ìœ ì—°í•œ ê¸°ê°„ ê³„ì‚°
          const sections = splitScriptWithDurations(script, audioDuration, 10, 4, 10);
          console.log(`[Pipeline] ìŠ¤í¬ë¦½íŠ¸ ${sections.length}ê°œ ì„¹ì…˜ìœ¼ë¡œ ë¶„í•  ì™„ë£Œ`);
          steps = updateStep(steps, 'split', { 
            status: 'completed', 
            endTime: new Date().toISOString(),
            result: { sectionCount: sections.length, audioDuration }
          });
          
          // Step 4: í´ë¦½ ì •ë³´ ì´ˆê¸°í™” (ê³„ì‚°ëœ ìœ ì—°í•œ ê¸°ê°„ ì‚¬ìš©)
          let clips = initializeClipsFromSections(sections);
          
          // Step 5: AIë¡œ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ìƒì„±
          steps = updateStep(steps, 'prompts', { status: 'processing', startTime: new Date().toISOString() });
          await updateJobStatus(job.id, {
            status: 'prompts',
            steps,
            clips,
            audioDuration,
          });
          
          console.log(`[Pipeline] AI ì˜ìƒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘...`);
          clips = await generateClipPrompts(clips, topic, category);
          steps = updateStep(steps, 'prompts', { status: 'completed', endTime: new Date().toISOString() });
          
          // Step 6: ì˜ìƒ ìƒì„± ìš”ì²­
          steps = updateStep(steps, 'render', { status: 'processing', startTime: new Date().toISOString() });
          await updateJobStatus(job.id, {
            status: 'render',
            steps,
            clips,
          });
          
          console.log(`[Pipeline] ${clips.length}ê°œ í´ë¦½ ì˜ìƒ ìƒì„± ìš”ì²­ ì‹œì‘...`);
          clips = await requestClipVideos(clips, 'higgsfield', selectedModel);
          
          // í´ë¦½ Job IDë“¤ì„ brewJobIdì— ì €ì¥
          const jobIds = clips.map(c => c.jobId).filter(Boolean).join(',');
          brewJobId = `multiclip|higgsfield|${selectedModel}|${audioDuration}|${jobIds}`;
          
          await updateJobStatus(job.id, {
            brewJobId,
            steps,
            clips,
          });
          
          console.log(`[Pipeline] ë©€í‹°í´ë¦½ ${clips.length}ê°œ ì‘ì—… ì‹œì‘ë¨`);
          
        } else {
          // ì¼ë°˜ ëª¨ë“œ: ë‹¨ì¼ í´ë¦½ ìƒì„±
          console.log(`[Pipeline] Higgsfield ì˜ìƒ ìƒì„± ìš”ì²­ ì‹œì‘ (ëª¨ë¸: ${selectedModel})...`);
          providerName = `Higgsfield (${selectedModel})`;
          const result = await requestHiggsfieldVideo({
            prompt: script,
            model: selectedModel,
            aspectRatio: '9:16',
            duration: 8,
          });
          brewJobId = result.jobId;
        }
      } else if (videoProvider === 'veo') {
        // Google Veo 3 ë°©ì‹ (ìµœì‹  AI ì˜ìƒ ìƒì„±)
        const selectedModel = veoModel || 'veo-3';
        const selectedDuration = veoDuration || 8; // ê¸°ë³¸ 8ì´ˆ
        
        if (useMultiClip && targetDuration && targetDuration > 15 && audioPath) {
          // ============================================
          // ğŸ¬ ë©€í‹°í´ë¦½ ëª¨ë“œ: Veo 3 ì›Œí¬í”Œë¡œìš°
          // ============================================
          console.log(`[Pipeline] Veo ë©€í‹°í´ë¦½ ëª¨ë“œ ì‹œì‘ (ëª©í‘œ: ${targetDuration}ì´ˆ, ëª¨ë¸: ${selectedModel})...`);
          providerName = `Veo MultiClip (${selectedModel})`;
          
          // Step 1: ì‘ì—… ë‹¨ê³„ ì´ˆê¸°í™”
          let steps = initializeSteps();
          steps = updateStep(steps, 'script', { status: 'completed', endTime: new Date().toISOString() });
          steps = updateStep(steps, 'tts', { status: 'completed', endTime: new Date().toISOString() });
          
          // Step 2: ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì •
          steps = updateStep(steps, 'split', { status: 'processing', startTime: new Date().toISOString() });
          console.log(`[Pipeline] ì˜¤ë””ì˜¤ ê¸¸ì´ ì¸¡ì • ì¤‘...`);
          const audioDuration = await getAudioDuration(audioPath);
          console.log(`[Pipeline] ì˜¤ë””ì˜¤ ê¸¸ì´: ${audioDuration}ì´ˆ`);
          
          // Step 3: ìŠ¤í¬ë¦½íŠ¸ ë¶„í•  ë° ìœ ì—°í•œ ê¸°ê°„ ê³„ì‚° (VeoëŠ” 4-8ì´ˆ)
          const sections = splitScriptWithDurations(script, audioDuration, 8, 4, 8);
          console.log(`[Pipeline] ìŠ¤í¬ë¦½íŠ¸ ${sections.length}ê°œ ì„¹ì…˜ìœ¼ë¡œ ë¶„í•  ì™„ë£Œ`);
          steps = updateStep(steps, 'split', { 
            status: 'completed', 
            endTime: new Date().toISOString(),
            result: { sectionCount: sections.length, audioDuration }
          });
          
          // Step 4: í´ë¦½ ì •ë³´ ì´ˆê¸°í™” (ê³„ì‚°ëœ ìœ ì—°í•œ ê¸°ê°„ ì‚¬ìš©)
          let clips = initializeClipsFromSections(sections);
          
          // Step 5: AIë¡œ ì˜ìƒ í”„ë¡¬í”„íŠ¸ ìƒì„±
          steps = updateStep(steps, 'prompts', { status: 'processing', startTime: new Date().toISOString() });
          await updateJobStatus(job.id, {
            status: 'prompts',
            steps,
            clips,
            audioDuration,
          });
          
          console.log(`[Pipeline] AI ì˜ìƒ í”„ë¡¬í”„íŠ¸ ìƒì„± ì‹œì‘...`);
          clips = await generateClipPrompts(clips, topic, category);
          steps = updateStep(steps, 'prompts', { status: 'completed', endTime: new Date().toISOString() });
          
          // Step 6: ì˜ìƒ ìƒì„± ìš”ì²­
          steps = updateStep(steps, 'render', { status: 'processing', startTime: new Date().toISOString() });
          await updateJobStatus(job.id, {
            status: 'render',
            steps,
            clips,
          });
          
          console.log(`[Pipeline] ${clips.length}ê°œ í´ë¦½ Veo ì˜ìƒ ìƒì„± ìš”ì²­ ì‹œì‘...`);
          clips = await requestClipVideos(clips, 'veo', selectedModel);
          
          // í´ë¦½ Job IDë“¤ì„ brewJobIdì— ì €ì¥
          const jobIds = clips.map(c => c.jobId).filter(Boolean).join(',');
          brewJobId = `multiclip|veo|${selectedModel}|${audioDuration}|${jobIds}`;
          
          await updateJobStatus(job.id, {
            brewJobId,
            steps,
            clips,
          });
          
          console.log(`[Pipeline] Veo ë©€í‹°í´ë¦½ ${clips.length}ê°œ ì‘ì—… ì‹œì‘ë¨`);
          
        } else {
          // ì¼ë°˜ ëª¨ë“œ: ë‹¨ì¼ í´ë¦½ ìƒì„±
          console.log(`[Pipeline] Google Veo ì˜ìƒ ìƒì„± ìš”ì²­ ì‹œì‘ (ëª¨ë¸: ${selectedModel}, ê¸¸ì´: ${selectedDuration}ì´ˆ, ìŒì„±í¬í•¨: ${veoWithAudio})...`);
          
          // Veo ìŒì„± í¬í•¨ ëª¨ë“œ: ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë‚˜ë ˆì´ì…˜ í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜
          let veoPrompt: string;
          if (veoWithAudio) {
            veoPrompt = `[Korean narration video] A warm and friendly Korean narrator reads the following script for a YouTube Shorts video targeting 50-60 year old Korean viewers:

"${script}"

Style: Professional health/lifestyle content, warm lighting, calming visuals, clear Korean speech with natural intonation. The narrator should speak slowly and clearly.`;
            providerName = `Veo (${selectedModel}) + AI ìŒì„± (${selectedDuration}ì´ˆ)`;
          } else {
            veoPrompt = script;
            providerName = `Veo (${selectedModel}) (${selectedDuration}ì´ˆ)`;
          }
          
          const result = await requestVeoVideo({
            prompt: veoPrompt,
            model: selectedModel,
            aspectRatio: '9:16',
            duration: selectedDuration,
          });
          brewJobId = result.jobId;
        }
      } else if (videoProvider === 'kling') {
        // Kling AI ë°©ì‹
        const selectedDuration = klingDuration || '5';
        console.log(`[Pipeline] Kling AI ì˜ìƒ ìƒì„± ìš”ì²­ ì‹œì‘ (ê¸¸ì´: ${selectedDuration}ì´ˆ)...`);
        providerName = `Kling (${selectedDuration}ì´ˆ)`;
        const result = await requestKlingVideo({
          prompt: script,
          aspect_ratio: '9:16',
          duration: selectedDuration,
        });
        brewJobId = result.jobId;
      } else if (videoProvider === 'google') {
        // Google Imagen + FFmpeg ë°©ì‹
        console.log(`[Pipeline] Google Imagen + FFmpeg ì˜ìƒ ìƒì„± ìš”ì²­ ì‹œì‘...`);
        providerName = 'Google';
        const result = await requestGoogleVideo({
          script,
          audioUrl: audioPath!, // Googleì€ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©
          aspectRatio: '9:16',
        });
        brewJobId = result.jobId;
      } else {
        // Runway ML ë°©ì‹ (ê¸°ë³¸ê°’)
        console.log(`[Pipeline] Runway ì˜ìƒ ìƒì„± ìš”ì²­ ì‹œì‘...`);
        providerName = 'Runway';
        const result = await requestRunwayVideo({
          script,
          audioUrl: audioUrl!,
          aspectRatio: '9:16',
        });
        brewJobId = result.jobId;
      }

      const updatedJob = await updateJobStatus(job.id, {
        brewJobId,
      });

      console.log(`[Pipeline] ${providerName} ì‘ì—… ìš”ì²­ ì™„ë£Œ: ${brewJobId}`);

      // ì‘ë‹µ ë°˜í™˜
      return NextResponse.json<PipelineResponse>({
        success: true,
        job: updatedJob!,
        message: `ì˜ìƒ ìƒì„± íŒŒì´í”„ë¼ì¸ì´ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ${providerName} ë Œë”ë§ ì™„ë£Œ í›„ ì—…ë¡œë“œê°€ ì§„í–‰ë©ë‹ˆë‹¤.`,
      });

    } catch (pipelineError) {
      // íŒŒì´í”„ë¼ì¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ ì‹œ Job ìƒíƒœ ì—…ë°ì´íŠ¸
      const errorMessage = pipelineError instanceof Error 
        ? pipelineError.message 
        : 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';

      const failedJob = await updateJobStatus(job.id, {
        status: 'failed',
        errorMessage,
      });

      console.error(`[Pipeline] ì˜¤ë¥˜ ë°œìƒ:`, pipelineError);

      return NextResponse.json<PipelineResponse>({
        success: false,
        job: failedJob!,
        message: errorMessage,
      }, { status: 500 });
    }

  } catch (error) {
    console.error('[Pipeline] ìš”ì²­ ì²˜ë¦¬ ì˜¤ë¥˜:', error);

    const errorMessage = error instanceof Error 
      ? error.message 
      : 'ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.';

    return NextResponse.json<ApiErrorResponse>(
      { success: false, error: errorMessage },
      { status: 500 }
    );
  }
}
